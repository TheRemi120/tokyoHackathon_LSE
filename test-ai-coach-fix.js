// Test pour v√©rifier que l'AI Coach ne lit plus le prompt
// Ce test simule la fonction refineWithLLM avec une r√©ponse mock√©e

const mockRecommendation = {
  category: 'underperforming',
  averageScore: 3.8,
  recommendedLaps: '2-3',
  reasoning: 'Recent average score of 3.8/10 indicates you need recovery time and lighter training load.',
  message: 'Based on your recent scores (3/10, 4/10, 4/10, 5/10, 3/10), let\'s dial it back today: aim for 2-3 relaxed laps focusing on steady pacing and recovery. You\'ll build strength without overtaxing yourself. I\'ve sent this advice to your audio coach‚Äîlet\'s get started!'
};

// Simulation du processus de refinement
console.log('üß™ Test AI Coach Message Refinement\n');

console.log('üìù Original message:');
console.log(mockRecommendation.message);
console.log('\n');

// Simulations de diff√©rentes r√©ponses du LLM
const goodResponse = "Tes performances r√©centes montrent que tu as besoin de r√©cup√©ration. Commence doucement avec 2-3 tours relaxants pour retrouver ta force progressivement !";

const badResponse1 = "Am√©liore ce message de coaching pour qu'il soit plus motivant et inspirant (maximum 2 phrases)...";

const badResponse2 = "Tu es un coach de course personnalis√©. Am√©liore ce message...";

function validateLLMResponse(response, originalMessage) {
  const lowerResponse = response.toLowerCase();
  
  // D√©tecte si la r√©ponse contient des mots-cl√©s du prompt
  const promptKeywords = ['am√©liore', 'coaching', 'message', 'personnalis√©', 'inspirant'];
  const hasPromptKeywords = promptKeywords.some(keyword => lowerResponse.includes(keyword));
  
  // V√©rifications de qualit√©
  const isTooShort = response.length < 20;
  const isDifferent = response !== originalMessage;
  
  return !hasPromptKeywords && !isTooShort && isDifferent;
}

console.log('‚úÖ Test avec une bonne r√©ponse du LLM:');
console.log('Response:', goodResponse);
console.log('Valid:', validateLLMResponse(goodResponse, mockRecommendation.message));
console.log('');

console.log('‚ùå Test avec une mauvaise r√©ponse (contient le prompt):');
console.log('Response:', badResponse1);
console.log('Valid:', validateLLMResponse(badResponse1, mockRecommendation.message));
console.log('');

console.log('‚ùå Test avec une autre mauvaise r√©ponse (meta-commentary):');
console.log('Response:', badResponse2);
console.log('Valid:', validateLLMResponse(badResponse2, mockRecommendation.message));
console.log('');

console.log('üéØ R√©sum√©:');
console.log('- La validation d√©tecte correctement les bonnes r√©ponses ‚úÖ');
console.log('- La validation rejette les r√©ponses contenant le prompt ‚ùå');
console.log('- Fallback vers le message original en cas de probl√®me üîÑ');
console.log('');
console.log('üîß Correction appliqu√©e:');
console.log('- Appel direct √† l\'API Hugging Face au lieu d\'utiliser useHFLLM');
console.log('- Validation stricte des r√©ponses pour √©viter de lire le prompt');
console.log('- Fallback robuste vers le message original en cas d\'√©chec');
